{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll learn how to use Keras-- an industry standard tool to build for quickly prototyping and training Deep Neural Networks.\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In the last lab, we learned how to build a small neural network from scratch, with only numpy to help.  While this was illuminating to help us learn how Artificial Neural Networks actually work, the construction of it was a bit too tedious for real-world data scientists to do on the job. Because of this, many great frameworks and libraries have been created to make it a simple task to build a neural network--Keras, and TensorFlow!\n",
    "<table>\n",
    "<tr>\n",
    "    <td><a href=\"https://www.tensorflow.org\"><img src='tensorflow-logo.png' height=100% width=100%></a></td>\n",
    "    <td><a href=\"https://keras.io/\"><img src='keras-logo.png' height=75% width=75%></a></td>\n",
    "</tr>\n",
    "</table>\n",
    "    \n",
    "## A Bit of Background\n",
    "\n",
    "If we're going to talk about Keras, we'll first need to talk about TensorFlow.  Keras and TensorFlow are both created and maintained by Google.  TensorFlow is a framework for building **_Static Computational Graphs_**, with a special emphasis on Deep Learning.  It provides much of the same functionality of numpy, but with a ton of common functionality needed for neural networks already created. Since working with TensorFlow can be a bit involved, Keras was invented as a high-level API for building Neural Networks with TensorFlow as a backend.  When you're using Keras, you're still using TensorFlow--you just don't have to write any TensorFlow code directly.\n",
    "\n",
    "The biggest benefit of using TensorFlow or Keras is that they provide [Auto Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) by default, which means that we don't actually have to handle Back Propagation, or caching any of the intermediate values during our forward propagation step so that we can use them later in back prop.  This is just one of the many reasons you'll love working with these tools!\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "We'll start by building a small MLP with Keras.  Run the cell below to import everything we'll need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Set\n",
    "\n",
    "In this lab, we'll be working with the MNIST dataset.  This dataset is a collection of handwritten digits, 0 through 9.  Each image is black and white, 28 pixel high, 28 pixels wide, and comes with a label.  From a historical perspective, this dataset has incredible significance to the Deep Learning community.  These days, it is treated as the \"Hello, World!\" of Deep Learning.  \n",
    "\n",
    "We'll have to do some basic data preparation in order to load in our data and get it in a shape usable by a Neural Network. \n",
    "\n",
    "In the cell below, call `mnist.load_data()` to store our data in the tuples we've laid out. You'll note that it's already been split into training and testing sets. \n",
    "\n",
    "**_NOTE:_** If this is your time ever running this on your computer, Keras will have to download the images from the internet.  This is a large dataset, and the download could take a while.  Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a sample image to see what one of the images looks like.  \n",
    "\n",
    "In the cell below, call `plt.imshow()` and pass in the first item from `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])\n",
    "\n",
    "print(\"Label: {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 5! A somewhat sloppy 5, but still a 5.  After a few short lines of code, we'll have built and trained a neural network that is able to recognize this as a 5.  Let's take a look at the way raw data for the first sample. \n",
    "\n",
    "In the cell below, access `X_train[0]` to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image is a 28x28 matrix.  In plain English, that means that it's an array containing 28 arrays, with each inner array containing 28 integer values between 0 and 255.  \n",
    "\n",
    "In order for us to feed this into a neural network, we'll need to reshape this from a matrix into a vector.  This is a basic step needed for most image preparation (except when we're building a Convolutional Neural Network--more on that in a later lab!).  \n",
    "\n",
    "In the cell below, we'll reshape `X_train` and `X_test` using their built in `.reshape()` methods. \n",
    "\n",
    "`X_train` contains 60,000 samples of shape `(28,28)`--so we'll reshape it to `(60000, 784)`.  We'll also method chain to cast the data inside of it from type `int8` to `float32`.  \n",
    "\n",
    "`X_test` contains only 10,000 samples, so we'll reshape that to `(10000, 784)`.  We'll also cast this to type `float32`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype(\"float32\")\n",
    "X_test = X_test.reshape(10000, 784).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're preprocessing our data, it's also a good idea to normalize it.  Normally, this would involve subtracting the mean and dividing by the standard deviation for each column, or using a `StandardScaler()` object from sklean.  However, since we're working with image data, we can use a quick hack and just divide every value by 255. \n",
    "\n",
    "In the cell below, normalize the data in `X_train` and `X_test` by dividing each by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully pre-processed our data, but we still need to deal with our labels, which are inside of `y_train` and `y_test`.  This is much easier--we need to change the data from its current categorical form to a one-hot encoded format.  Keras makes this easy by providing a `to_categorical()` function inside of its `utils` module.\n",
    "\n",
    "In the cell below, call `keras.utils.to_categorical()` and pass in the variable containing the labels to be reshaped as the first parameter.  For the second parameter, pass in the number of categories the data contains--in this case, `10`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now successfully preprocessed our data so that it's ready for use in a neural network!\n",
    "\n",
    "## Building a Neural Network with Keras\n",
    "\n",
    "Let's start by building our neural network. This means we need to create a `Sequential()` object.  This is an empty neural network that hasn't had any layers added to it yet.  \n",
    "\n",
    "Our workflow in Keras will consist of of the following:\n",
    "\n",
    "* Create a `Sequential` object\n",
    "* Add the layers we want to our `Sequential` object. When defining the first hidden layer, we'll also define our input layer.  \n",
    "* Compile our model\n",
    "* Fit our model\n",
    "* Examine the results!\n",
    "\n",
    "Let's get started! In the cell below, create a `Sequential()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add layers to our model by passing them in to the model's `.add()` method.  There are a ton of different kinds of layers we can add to our model, but for this lab we'll keep it simple and only use a single `Dense()` layer. \n",
    "\n",
    "\n",
    "\n",
    "There are several parameters we'll need to pass in to the `Dense()` layer we're creating at each step, to define exactly how we want our Dense layer to be shaped.  We'll need to tell the first layer the following things:\n",
    "\n",
    "* The number of neurons in the layer\n",
    "* The activation function of the layer (although this can also be added using `model.add()`, it's easier to just pass it in as a keyword parameter when creating the `Dense` layer)\n",
    "* If the layer we're creating is the first hidden layer, we'll also need to tell it the shape of the input layer.  In this case, we'll do that by passing in the keyword parameter `input_shape=(784,)`.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call `model.add()` and pass in a `Dense` layer with `25` neurons.  Also set the activation to `\"sigmoid\"`, and the input shape to `(784,)`\n",
    "* Call `model.add()` again to create out output layer.  This time, pass in a `Dense` layer with `10` neurons (one for each possible class that it can predict).  This time, set the activation to `\"softmax\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(25, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! With 2 lines of code, we've now created a complex neural network capable of reading human handwriting!\n",
    "\n",
    "Let's look at the structure of the model we've just created. \n",
    "\n",
    "In the cell below, call `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                19625     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 19,885\n",
      "Trainable params: 19,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the summary of our model doesn't show the input layer by default. This is a common convention in Deep Learning, since the input layer doesn't contain any **_trainable parameters_**, meaning that it doesn't really do anything.  \n",
    "\n",
    "Also note the number of trainable parameters in our network--19,885! Although this may seem like a lot, its still quite small compared to typical Artificial Neural Networks used in Deep Learning.  \n",
    "\n",
    "Now, we need to compile our model.  This is a step unique to Keras, where the neural network we've created is actually created as a static computational graph in TensorFlow.  During this step, we'll need to pass in certain some information. Specifically:\n",
    "\n",
    "* We'll need to specify the loss.  Since this is a multi-categorical problem, we'll set the `loss` to `\"categorical_crossentropy\"`.\n",
    "* We'll also need to tell the model what type of optimizer we'll use.  In this case, set the `optimzer` to `\"SGD\"`, which is short for **_Stochastic Gradient Descent_**.\n",
    "* Finally, we'll need to tell the model what metrics we want back.  In this case, we'll pass in `[\"accuracy\"]` to the `metrics` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the fun part--we can actually fit our model, and see how well it does! \n",
    "\n",
    "Just like the last step, there are multiple parameters we'll need to pass in, such as:\n",
    "\n",
    "* `X_train` and `y_train`\n",
    "* a `batch_size` of `32`\n",
    "* The number of `epochs`, which we'll set to `10`.\n",
    "* Set `verbose` to `1`, so that we'll get messages as the model trains. \n",
    "* We'll also pass in `validation_data` to the tuple`(X_test, y_test)`.  With this, the model will compute the accuracy on our validation set at the end of each epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 1.7039 - acc: 0.6209 - val_loss: 1.1833 - val_acc: 0.7776\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.9545 - acc: 0.8032 - val_loss: 0.7601 - val_acc: 0.8426\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.6826 - acc: 0.8479 - val_loss: 0.5876 - val_acc: 0.8714\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.5567 - acc: 0.8691 - val_loss: 0.4968 - val_acc: 0.8834\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.4848 - acc: 0.8819 - val_loss: 0.4418 - val_acc: 0.8920\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.4387 - acc: 0.8894 - val_loss: 0.4048 - val_acc: 0.8968\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.4065 - acc: 0.8949 - val_loss: 0.3785 - val_acc: 0.9022\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.3826 - acc: 0.8987 - val_loss: 0.3579 - val_acc: 0.9058\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3642 - acc: 0.9018 - val_loss: 0.3425 - val_acc: 0.9087\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3493 - acc: 0.9050 - val_loss: 0.3300 - val_acc: 0.9104\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of 91%--that's pretty good!\n",
    "\n",
    "\n",
    "## Visualizing our Results\n",
    "\n",
    "Calling `model.fit()` in Keras has the added benefit of returning a `History` object that contains all the statistics we saw above, that were printed out above live during the training.  It's a good habit to visualize our loss and accuracy, as well as our validation loss and accuracy.  \n",
    "\n",
    "In the cell below, examine `model.history` to see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [1.1832673015594481,\n",
       "  0.7600571701049804,\n",
       "  0.5876122484207154,\n",
       "  0.4968342936515808,\n",
       "  0.4417539575815201,\n",
       "  0.404845161652565,\n",
       "  0.378500884103775,\n",
       "  0.3579380490064621,\n",
       "  0.34248225321769715,\n",
       "  0.3300070330500603],\n",
       " 'val_acc': [0.7776,\n",
       "  0.8426,\n",
       "  0.8714,\n",
       "  0.8834,\n",
       "  0.892,\n",
       "  0.8968,\n",
       "  0.9022,\n",
       "  0.9058,\n",
       "  0.9087,\n",
       "  0.9104],\n",
       " 'loss': [1.7038502355893452,\n",
       "  0.9544538551648458,\n",
       "  0.6826119331200917,\n",
       "  0.556686835193634,\n",
       "  0.48483793279329934,\n",
       "  0.438665455253919,\n",
       "  0.40647188479105634,\n",
       "  0.3826390848318736,\n",
       "  0.3641694936672846,\n",
       "  0.3493064123193423],\n",
       " 'acc': [0.6208833333333333,\n",
       "  0.8032333333333334,\n",
       "  0.84785,\n",
       "  0.8690666666666667,\n",
       "  0.8818666666666667,\n",
       "  0.88945,\n",
       "  0.8949333333333334,\n",
       "  0.8987166666666667,\n",
       "  0.9017666666666667,\n",
       "  0.90495]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It's just basic python dictionary with the following keys:\n",
    "\n",
    "* `val_loss`\n",
    "* `val_acc`\n",
    "* `loss`\n",
    "* `acc`\n",
    "\n",
    "Let's plot our Accuracy and Validation Accuracy on the same plot, so that we check if the model is overfitting at all. \n",
    "\n",
    "In the cell below, create a visualization using Matplotlib, and plot `val_acc` and `acc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = [i for i in range(1, 11)]\n",
    "plt.figure()\n",
    "plt.plot(epochs, model.history['val_acc'])\n",
    "plt.plot(epochs, model.history['acc'])\n",
    "plt.title(\"Model Accuracy by Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"val_acc\", \"acc\"], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question:_** Do you detect any overfitting or underfitting in the model? How can you tell? Explain your answer.\n",
    "\n",
    "Write your answer below this line:\n",
    "______________________________________________________________________________________________________________________\n",
    "\n",
    "The model seems to have a high overall validation accuracy score, which means the model is not underfit.  The training accuracy and testing accuracy are also very close, suggesting that the model has not yet begun to overfit the data.  However, this would likely happen if we drastically increased the size of the network, or continued training on the data for many more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bigger Model\n",
    "\n",
    "Now that you've cut your teeth in Keras, let's build a bigger model! For the remainder of this notebook, your task it build your own neural network in Keras from start to finish.  You do not need to repeat the data preprocessing steps done at the beginning of this notebook, since that data is still available to us. \n",
    "\n",
    "In the cells below, your task is to:\n",
    "* Build and train a bigger model for classification on the MNIST dataset. \n",
    "* Visualize your results to check for overfitting and underfitting.\n",
    "\n",
    "When building your MLP below, consider the following choices you'll need to make:\n",
    "\n",
    "* How many layers will your model have?\n",
    "* How many neurons will each layer have?\n",
    "* What activation function will your model use?\n",
    "\n",
    "We'll explore these topics in depth in a later lab, but it never hurts to learn through experimentation. \n",
    "\n",
    "**_NOTE:_**  The larger your network, the longer the training will take.  Don't build something too big if you don't have access to GPU-backed training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 0.2917 - acc: 0.9182 - val_loss: 0.1597 - val_acc: 0.9528\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 26s 440us/step - loss: 0.1283 - acc: 0.9624 - val_loss: 0.1268 - val_acc: 0.9623\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 0.0897 - acc: 0.9727 - val_loss: 0.1084 - val_acc: 0.9677\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 27s 448us/step - loss: 0.0684 - acc: 0.9789 - val_loss: 0.1037 - val_acc: 0.9692\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 27s 449us/step - loss: 0.0528 - acc: 0.9841 - val_loss: 0.0885 - val_acc: 0.9744\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 27s 448us/step - loss: 0.0453 - acc: 0.9860 - val_loss: 0.0984 - val_acc: 0.9705\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 27s 451us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.0882 - val_acc: 0.9755\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 28s 472us/step - loss: 0.0302 - acc: 0.9905 - val_loss: 0.0934 - val_acc: 0.9751\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 27s 451us/step - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0969 - val_acc: 0.9733\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 28s 465us/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.0905 - val_acc: 0.9756\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 28s 471us/step - loss: 0.0204 - acc: 0.9938 - val_loss: 0.0996 - val_acc: 0.9716\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 26s 441us/step - loss: 0.0210 - acc: 0.9930 - val_loss: 0.1079 - val_acc: 0.9711\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 29s 486us/step - loss: 0.0184 - acc: 0.9940 - val_loss: 0.0935 - val_acc: 0.9782\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 24s 393us/step - loss: 0.0164 - acc: 0.9949 - val_loss: 0.1134 - val_acc: 0.9726\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 24s 396us/step - loss: 0.0159 - acc: 0.9951 - val_loss: 0.1221 - val_acc: 0.9713\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 26s 431us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.1008 - val_acc: 0.9752\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 25s 413us/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.1135 - val_acc: 0.9733\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 27s 448us/step - loss: 0.0153 - acc: 0.9951 - val_loss: 0.1199 - val_acc: 0.9730\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 28s 463us/step - loss: 0.0094 - acc: 0.9971 - val_loss: 0.1146 - val_acc: 0.9724\n",
      "Epoch 20/20\n",
      " 7488/60000 [==>...........................] - ETA: 23s - loss: 0.0245 - acc: 0.9931"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100, activation='tanh', input_shape=(784,)))\n",
    "model2.add(Dense(75, activation='tanh'))\n",
    "model2.add(Dense(25, activation='tanh'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_2 = model2.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_2 = [i for i in range(1, 21)]\n",
    "plt.figure()\n",
    "plt.plot(epochs_2, history_2.history['val_acc'])\n",
    "plt.plot(epochs_2, history_2.history['acc'])\n",
    "plt.title(\"Model Accuracy by Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"val_acc\", \"acc\"], loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we learned how to build a basic model in Keras, and gained some valuable experience with the general Deep Learning workflow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
